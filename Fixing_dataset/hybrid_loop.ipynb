{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43586655-7c90-4d07-b6a1-d1a33af73c42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 15:54:13.343085: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-18 15:54:13.977683: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-18 15:54:15.577725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-18 15:54:15.766619: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-18 15:54:15.838050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-18 15:54:16.219213: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-18 15:54:24.956754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import pprint\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from IPython.display import display, HTML\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdb347a-21bb-481b-9958-d6a9ab9b7bf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94d83a4-e261-4e6f-b7e6-b07beac44a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading API Key from file\n",
    "API_KEY = open(\"open_ai_API.txt\", \"r\").read().strip()\n",
    "openai.api_key = API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad8f4dc-feb8-42e0-9b68-bdaf7adccc0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "561271be-b191-4597-803e-410925595ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_dataset(\"generaleoley/manim-codegen\", split='train')\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d23b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import llm_tools, tools_local\n",
    "\n",
    "lt = llm_tools()\n",
    "tl = tools_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e12fa402-14ef-4b39-9031-7789d6f0530e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# #Choosing a specific code from the dataset by giving value of 'reference', change value for each code \n",
    "\n",
    "# sorted_set = []\n",
    "\n",
    "# for j in range(3):\n",
    "    \n",
    "#     reference = j\n",
    "#     code_string = data[reference]['answer']\n",
    "#     formatted_code_string = code_string.replace('\\\\n', '\\n')\n",
    "#     # print(formatted_code_string)\n",
    "\n",
    "#     # Prepare the messages for the Chat API which will be used later for cosine similarity\n",
    "#     messages = [\n",
    "#         # {\"role\": \"system\", \"content\": \"You are a helpful assistant that performs semantic analysis.\"},\n",
    "#         {\"role\": \"user\", \"content\": f\"Explain the purpose of this code wihtout getting into technical details, \\n\\nParagraph 1:\\n{formatted_code_string}\"},\n",
    "\n",
    "#         {\"role\": \"user\", \"content\": f\"In detail, explain what is happening with the visuals,  \\n\\nParagraph 1:\\n{formatted_code_string} \"}\n",
    "#     ]\n",
    "\n",
    "#     # Make the API call\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",  # Or another model as appropriate\n",
    "#         messages=messages,\n",
    "#         max_tokens=700,  # Adjust according to your needs\n",
    "#         temperature=0.01  # Adjust for more creativity or precision\n",
    "#     )\n",
    "#     open_gen = response.choices[0].message.content\n",
    "#     # printer(open_gen)\n",
    "    \n",
    "    \n",
    "#     model1 = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "\n",
    "# ## Convert to function ####\n",
    "\n",
    "# ## Ask gpt if it fits ####\n",
    "\n",
    "# ### Remove correct pairs #####\n",
    "\n",
    "#     top_similarities = lt.similarity_ranking(data,model1, open_gen, 10)\n",
    "    \n",
    "#     all_scores = [item['score'] for item in top_similarities]\n",
    "#     all_indices = [item['index'] for item in top_similarities]\n",
    "#     all_queries = [item['query'] for item in top_similarities]\n",
    "        \n",
    "#     messages2 = lt.create_message_for_comparison(code_string, all_queries)\n",
    "\n",
    "#     # Make the API call to GPT-4o-mini for comparison\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=messages2,\n",
    "#         max_tokens=700,\n",
    "#         temperature=0.01\n",
    "#     )\n",
    "\n",
    "#     # Extract and return the most similar query from the response\n",
    "#     response_content = response.choices[0].message.content\n",
    "#     # printer(response_content)\n",
    "    \n",
    "#     searched_index = tl.searching(response_content, reference, data)\n",
    "#     print(searched_index)\n",
    "\n",
    "#     sorted_set.append(searched_index)\n",
    "\n",
    "\n",
    "# end_time = time.time()\n",
    "# print(f\"Exectution time :\",end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3cb4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d487d205-6af4-4fa4-a63d-ac1269dae947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/h/Hassan.Mo/llm_env/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0, Iteration 0, Searched Index: [(0, 771)]\n",
      "Run 0, Iteration 1, Searched Index: [(1, 990)]\n",
      "Run 0, Iteration 2, Searched Index: []\n",
      "Completed Run 0\n",
      "Run 1, Iteration 0, Searched Index: [(0, 771)]\n",
      "Run 1, Iteration 1, Searched Index: [(1, 990)]\n",
      "Run 1, Iteration 2, Searched Index: [(2, 804)]\n",
      "Completed Run 1\n",
      "Run 2, Iteration 0, Searched Index: [(0, 771)]\n",
      "Run 2, Iteration 1, Searched Index: [(1, 990)]\n",
      "Run 2, Iteration 2, Searched Index: [(2, 599)]\n",
      "Completed Run 2\n",
      "Run 3, Iteration 0, Searched Index: [(0, 506)]\n",
      "Run 3, Iteration 1, Searched Index: [(1, 990)]\n",
      "Run 3, Iteration 2, Searched Index: [(2, 804)]\n",
      "Completed Run 3\n",
      "Execution time: 332.8417229652405\n",
      "Results from Run 1: [[(0, 771)], [(1, 990)], []]\n",
      "Results from Run 2: [[(0, 771)], [(1, 990)], [(2, 804)]]\n",
      "Results from Run 3: [[(0, 771)], [(1, 990)], [(2, 599)]]\n",
      "Results from Run 4: [[(0, 506)], [(1, 990)], [(2, 804)]]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Initialize a list to store results of all runs\n",
    "all_runs_searched_indices = []\n",
    "all_runs_top_similarities = []  # To store the top similarities for each run\n",
    "\n",
    "# Run the loop 5 times\n",
    "for run in range(4):\n",
    "    \n",
    "    sorted_set = []  # This will store the result of each run\n",
    "    run_top_similarities = []  # To store top similarities for the current run\n",
    "\n",
    "    for i in range(3):  # Loop through each code snippet in the dataset\n",
    "        \n",
    "        reference = i\n",
    "        code_string = data[reference]['answer']\n",
    "        formatted_code_string = code_string.replace('\\\\n', '\\n')\n",
    "\n",
    "        # Prepare the messages for the Chat API\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Explain the purpose of this code without getting into technical details, \\n\\nParagraph 1:\\n{formatted_code_string}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"In detail, explain what is happening with the visuals,  \\n\\nParagraph 1:\\n{formatted_code_string} \"}\n",
    "        ]\n",
    "\n",
    "        # Make the API call for explanation\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            max_tokens=700,\n",
    "            temperature=0.01\n",
    "        )\n",
    "\n",
    "        open_gen = response.choices[0].message.content\n",
    "\n",
    "        model1 = SentenceTransformer('all-MiniLM-L6-v2').to(device) \n",
    "        \n",
    "        top_similarities = lt.similarity_ranking(data, model1, open_gen, 5)\n",
    "\n",
    "        # Store top scores, indices, and queries\n",
    "        all_scores = [item['score'] for item in top_similarities]\n",
    "        all_indices = [item['index'] for item in top_similarities]\n",
    "        all_queries = [item['query'] for item in top_similarities]\n",
    "\n",
    "        # Save top indices and scores for the current run\n",
    "        run_top_similarities.append({\n",
    "            'iteration': i,\n",
    "            'indices': all_indices,\n",
    "            'scores': all_scores\n",
    "        })\n",
    "\n",
    "\n",
    "        # Prepare messages for comparison\n",
    "        messages2 = lt.create_message_for_comparison(code_string, all_queries)\n",
    "\n",
    "        # Make the API call for comparison\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages2,\n",
    "            max_tokens=700,\n",
    "            temperature=0.01\n",
    "        )\n",
    "\n",
    "\n",
    "        # Extract the most similar query from the response\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "\n",
    "        # Perform searching based on response content and reference\n",
    "        searched_index = tl.searching(response_content, reference, data)\n",
    "        print(f\"Run {run}, Iteration {i}, Searched Index: {searched_index}\")\n",
    "\n",
    "\n",
    "        # Append the result to sorted_set\n",
    "        sorted_set.append(searched_index)\n",
    "\n",
    "\n",
    "    # Store the result of the current run\n",
    "    all_runs_searched_indices.append(sorted_set)\n",
    "    all_runs_top_similarities.append(run_top_similarities)  # Append the run's top similarities\n",
    "    print(f\"Completed Run {run}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "# Output execution time\n",
    "print(f\"Execution time: {end_time - start_time}\")\n",
    "\n",
    "\n",
    "# Accessing the searched indices for each run\n",
    "for run_idx, run_results in enumerate(all_runs_searched_indices):\n",
    "    print(f\"Results from Run {run_idx + 1}: {run_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd272e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Similarities from Run 1:\n",
      "Iteration 0 - Indices: [771, 802, 490, 620, 581], Scores: [0.8299391269683838, 0.8209885954856873, 0.8169922232627869, 0.8050893545150757, 0.7997460961341858]\n",
      "Iteration 1 - Indices: [990, 986, 22, 25, 987], Scores: [0.6782309412956238, 0.6243355870246887, 0.6078952550888062, 0.6071741580963135, 0.606212854385376]\n",
      "Iteration 2 - Indices: [751, 745, 599, 804, 540], Scores: [0.7789125442504883, 0.7719681859016418, 0.7715654373168945, 0.7652341723442078, 0.7648677825927734]\n",
      "Top Similarities from Run 2:\n",
      "Iteration 0 - Indices: [512, 490, 581, 771, 631], Scores: [0.8192092180252075, 0.8120673894882202, 0.8066164255142212, 0.8017783164978027, 0.8006011843681335]\n",
      "Iteration 1 - Indices: [990, 986, 22, 25, 954], Scores: [0.6578432321548462, 0.614272952079773, 0.6069788932800293, 0.6032810807228088, 0.5942022800445557]\n",
      "Iteration 2 - Indices: [599, 804, 430, 751, 520], Scores: [0.7449238300323486, 0.7412062883377075, 0.7324252128601074, 0.7303513288497925, 0.7269930243492126]\n",
      "Top Similarities from Run 3:\n",
      "Iteration 0 - Indices: [490, 771, 506, 631, 758], Scores: [0.8140824437141418, 0.80391526222229, 0.8009101152420044, 0.7938627600669861, 0.7895000576972961]\n",
      "Iteration 1 - Indices: [990, 986, 22, 25, 989], Scores: [0.6872015595436096, 0.6562894582748413, 0.6380523443222046, 0.6368881464004517, 0.6243185997009277]\n",
      "Iteration 2 - Indices: [751, 599, 581, 745, 540], Scores: [0.7883020639419556, 0.7818796634674072, 0.781838059425354, 0.7813478112220764, 0.775542676448822]\n",
      "Top Similarities from Run 4:\n",
      "Iteration 0 - Indices: [490, 506, 758, 771, 581], Scores: [0.829926073551178, 0.8263996839523315, 0.8176953792572021, 0.8172361850738525, 0.8167999982833862]\n",
      "Iteration 1 - Indices: [990, 986, 22, 25, 989], Scores: [0.6620253324508667, 0.6301589012145996, 0.6057044267654419, 0.6048010587692261, 0.60274338722229]\n",
      "Iteration 2 - Indices: [751, 745, 804, 599, 540], Scores: [0.769554853439331, 0.7681593298912048, 0.7658172249794006, 0.7631264925003052, 0.7614215016365051]\n"
     ]
    }
   ],
   "source": [
    "# Accessing the top similarities for each run\n",
    "for run_idx, run_top in enumerate(all_runs_top_similarities):\n",
    "    print(f\"Top Similarities from Run {run_idx + 1}:\")\n",
    "    for item in run_top:\n",
    "        print(f\"Iteration {item['iteration']} - Indices: {item['indices']}, Scores: {item['scores']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06bf9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python llm_env",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
